{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "2e66a764-7f70-4f99-939e-acd290439986",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "## User-Defined Functions (UDFs) in Apache Spark with Python\n",
    "\n",
    "User-Defined Functions (UDFs) in Apache Spark empower you to extend Spark's functionality by incorporating custom logic written in Python (or other languages). This lesson explores UDFs, highlighting their benefits and drawbacks, and comparing standard UDFs with Pandas UDFs for performance.\n",
    "\n",
    "\n",
    "**What are UDFs?**\n",
    "\n",
    "UDFs are functions you define and register with the Spark session. They can then be applied to columns within Spark DataFrames, enabling custom data transformations and calculations that are not built into Spark's core functionality.  This allows you to leverage your existing Python code or create specialized functions tailored to your specific data processing needs.\n",
    "\n",
    "\n",
    "**Creating and Using Standard UDFs**\n",
    "\n",
    "The simplest way to create a UDF in Spark is using the `spark.udf.register()` method. This registers a Python function that can be called within Spark DataFrame operations.\n",
    "\n",
    "This code defines a simple UDF `age_in_five_years`, registers it with Spark, and applies it to the \"age\" column, creating a new column \"age_in_five_years\".\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "e895a612-c35b-432b-9e84-a5f650194d02",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "from pyspark.sql.functions import col\n",
    "\n",
    "# Sample DataFrame\n",
    "data = [(\"Alice\", 25), (\"Bob\", 30), (\"Charlie\", 28)]\n",
    "columns = [\"name\", \"age\"]\n",
    "df = spark.createDataFrame(data, columns)\n",
    "\n",
    "# Define a UDF to calculate the age in years from now\n",
    "def age_in_five_years(age):\n",
    "    return age + 5\n",
    "\n",
    "# Register the UDF\n",
    "age_in_five_years_udf = spark.udf.register(\"age_in_five_years\", age_in_five_years)\n",
    "\n",
    "# Apply the UDF to the DataFrame\n",
    "df = df.withColumn(\"age_in_five_years\", age_in_five_years_udf(col(\"age\")))\n",
    "\n",
    "# Show the result\n",
    "df.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "9f2ed0d6-a3d9-438c-b40a-ff21dd401da1",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "**Pandas UDFs:  A Performance Boost**\n",
    "\n",
    "Pandas UDFs offer significant performance advantages over standard UDFs, especially when dealing with complex operations or large datasets. They leverage the power of Pandas' vectorized operations to process data in chunks, minimizing the overhead of transferring data back and forth between Spark and Python.\n",
    "\n",
    "The `@pandas_udf` decorator indicates a Pandas UDF.  The function now operates on a Pandas Series (`age_series`), allowing for vectorized computations.  The `\"int\"` argument specifies the return type.\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "3427d125-3023-4ca7-ab53-0c72449f682f",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "from pyspark.sql.functions import pandas_udf\n",
    "import pandas as pd\n",
    "\n",
    "# Sample DataFrame (same as before)\n",
    "data = [(\"Alice\", 25), (\"Bob\", 30), (\"Charlie\", 28)]\n",
    "columns = [\"name\", \"age\"]\n",
    "df = spark.createDataFrame(data, columns)\n",
    "\n",
    "# Define a Pandas UDF to calculate age in five years (vectorized)\n",
    "@pandas_udf(\"int\")\n",
    "def age_in_five_years_pandas(age_series: pd.Series) -> pd.Series:\n",
    "    return age_series + 5\n",
    "\n",
    "# Apply the Pandas UDF\n",
    "df = df.withColumn(\"age_in_five_years\", age_in_five_years_pandas(col(\"age\")))\n",
    "\n",
    "# Show the result\n",
    "display(df)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "899dbdd8-49e5-4dd8-92eb-4d2f525fb429",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "**Understanding Python UDF Execution in Spark**\n",
    "\n",
    "When you execute a Python UDF within a Spark job, a fascinating interplay of processes occurs behind the scenes.  Let's break down what's happening:\n",
    "\n",
    "1. **Data Partitioning:** The Spark DataFrame is divided into multiple partitions, distributed across the worker nodes (executors) in the cluster.  This parallelizes processing.\n",
    "\n",
    "2. **UDF Serialization:** Your Python UDF code needs to be serialized (converted into a byte stream) and transmitted to each executor.  This ensures that every worker has a copy of the function to execute.  The serialization process uses Python's `pickle` module by default. This adds overhead, especially for large UDFs.\n",
    "\n",
    "3. **Data Transfer:**  Data from each partition is sent, one partition at a time, from the executors to the Python worker processes running on those executors.  This transfer happens through Spark's network communication infrastructure.  This transfer is another potential bottleneck, especially for large datasets or complex data structures.\n",
    "\n",
    "4. **Python Worker Execution:** On each executor, a Python worker process is launched (or reused from a pool). This worker receives the serialized UDF and the data for its assigned partition.  The UDF is then executed, processing the data in a partition.  Because it’s Python code, this stage can be relatively slow.\n",
    "\n",
    "5. **Pandas UDF Optimization:**  In the case of a Pandas UDF, the data is not processed row-by-row.  Instead, it leverages the power of Pandas to process the whole dataset (or a large chunk of it) at once using vectorized operations. The Pandas operations within the UDF are done in memory.  This is a massive optimization over standard UDFs which process one row at a time.\n",
    "\n",
    "6. **Result Aggregation:**  After the UDF execution on each partition, the results are sent back to the driver node, where they are aggregated (combined) into the final output DataFrame.  Again, this data transfer can become a performance bottleneck with very large datasets.\n",
    "\n",
    "7. **Garbage Collection:**  Spark's garbage collection (automated memory management) periodically cleans up unused memory.  Python's garbage collector also plays a role, further impacting performance.  Efficient garbage collection is crucial for minimizing latency.\n",
    "\n",
    "\n",
    "**Why this matters:**\n",
    "\n",
    "Understanding these steps highlights potential performance bottlenecks:\n",
    "\n",
    "* **Serialization/Deserialization:** Serialization/deserialization overhead increases with UDF size and data volume.\n",
    "* **Network Transfer:** Data transfer between executors and Python workers can be slow, especially across a network.\n",
    "* **Python Execution Speed:** Python is generally slower than JVM-based languages (like Java or Scala).  This difference is why Pandas UDFs are so much faster; they significantly reduce the amount of data moving between Python and JVM.\n",
    "\n",
    "**Optimizations:**\n",
    "\n",
    "* **Pandas UDFs:** Use Pandas UDFs whenever possible for improved performance, especially with large datasets and computationally intensive operations. The data transfer is reduced to just one transfer for the entire dataset chunk which is then processed very quickly in the Pandas vectorized operations.\n",
    "* **Minimize Data Transfer:**  Efficient data structures and careful column selection can reduce the volume of data transferred.\n",
    "* **Efficient UDFs:** Write concise and optimized UDFs to minimize execution time.\n",
    "* **Broadcast Variables:** For smaller datasets needed by the UDF, consider using broadcast variables to avoid redundant data transfers to each executor.\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "b7a6239f-8f0d-4e28-a727-4a7d50c89100",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "**Code Overview: Combining Machine Learning and Distributed Data Processing**\n",
    "\n",
    "This Python script demonstrates a common workflow in data science, where we combine the power of traditional machine learning algorithms (from scikit-learn) with the scalability of distributed data processing (using Apache Spark). The primary goal is to train a classification model using scikit-learn and then use that model to make predictions on a larger dataset managed by Spark, while also calculating common classification metrics.\n",
    "\n",
    "**Detailed Breakdown:**\n",
    "\n",
    "1.  **Sample Data Generation (using NumPy and Pandas)**:\n",
    "    *   **Purpose**: To create a synthetic dataset for training and testing our model.\n",
    "    *   **Implementation**:\n",
    "        *   `np.random.seed(42)`: Sets a random seed for NumPy's random number generator. This ensures reproducibility, meaning that each time the code is run the same random data will be generated. This is important for consistent results and debugging.\n",
    "        *   `num_samples = 1000`: Defines the size of the dataset (number of data points).\n",
    "        *   `feature_1 = np.random.rand(num_samples) * 10` and `feature_2 = np.random.rand(num_samples) * 10`: Generates two numerical features, with random values uniformly distributed between 0 and 10. These features will be the input for our model.\n",
    "        *   `target = np.where(feature_1 + feature_2 > 10, \"Class_A\", \"Class_B\")`: Creates a categorical target label based on a simple rule. If the sum of `feature_1` and `feature_2` is greater than 10, the target is assigned \"Class_A\", otherwise it’s \"Class_B\". This provides a binary classification problem, where our model tries to predict whether a data point is \"Class_A\" or \"Class_B\" based on its two features.\n",
    "        *   `data = pd.DataFrame(...)`: Creates a Pandas DataFrame to store our synthetic data, which is a tabular data structure designed for data analysis. Each column is a feature or the target, and each row is a data point.\n",
    "\n",
    "2.  **Scikit-learn Model Training (using RandomForestClassifier)**:\n",
    "    *   **Purpose**: To train a classification model on a portion of the generated data. This model will later be used for inference.\n",
    "    *   **Implementation**:\n",
    "        *   `X = data[['feature_1', 'feature_2']]` and `y = data['target']`: Separates the features (`X`) and target variable (`y`) from the Pandas DataFrame.\n",
    "        *   `X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)`: Splits the data into training and testing sets. The training set (80% of the data) will be used to train the model, while the testing set (20% of the data) will be used to evaluate the performance of the trained model. The random state ensures reproducible split for a given seed.\n",
    "        *   `model = RandomForestClassifier(n_estimators=100, random_state=42)`: Initializes a Random Forest classifier, a popular ensemble learning method. The parameters `n_estimators=100` indicates that the model will use 100 decision trees.\n",
    "        *   `model.fit(X_train, y_train)`: Trains the model using the training features and target labels. The model learns from the training data the optimal patterns to predict the target.\n",
    "        *   `predictions = model.predict(X_test)`: The model makes predictions on the test data.\n",
    "        *    `accuracy = accuracy_score(y_test, predictions)`: Calculates the accuracy score between the model's predictions and the actual values of the test dataset, this will be printed to show the quality of the trained model.\n",
    "\n",
    "3.  **Apache Spark Setup and Data Preparation**:\n",
    "    *   **Purpose**: To create a Spark environment and bring the data into a distributed, scalable framework.\n",
    "    *   **Implementation**:\n",
    "        *   `schema = StructType(...)`: Defines the schema of the Spark DataFrame. This schema is needed to tell Spark what types of columns are expected in the DataFrame, enabling Spark to properly distribute and process the data.\n",
    "        *   `spark_df = spark.createDataFrame(data, schema=schema)`: Creates a Spark DataFrame from the Pandas DataFrame, using the specified schema. The data is now stored in the Spark cluster, and can be used for distributed processing.\n",
    "        *   `spark_df.show(5)`: Displays the first five rows of the Spark DataFrame for inspection.\n",
    "\n",
    "4.  **Pandas UDF for Scalable Inference**:\n",
    "    *   **Purpose**: To apply the trained scikit-learn model to the entire Spark DataFrame using a distributed approach.\n",
    "    *   **Implementation**:\n",
    "        *   `@pandas_udf(\"string\", functionType=\"pandas\")`: Decorates the `predict_with_model` function to be a Pandas UDF. This allows you to use the code with pandas to process a batch of rows from a Spark dataframe.\n",
    "        *   `def predict_with_model(feature_1: pd.Series, feature_2: pd.Series) -> pd.Series:`: This defines the function. It takes pandas Series as inputs, and it should return a pandas Series object as output.\n",
    "            *   `input_data = pd.DataFrame({'feature_1': feature_1, 'feature_2': feature_2})`: Receives two pandas Series as input and creates a pandas DataFrame object.\n",
    "            *    `predictions = model.predict(input_data)`: Uses the scikit-learn `model` to make predictions on the input data, making use of vectorization features of the pandas library to make predictions faster than with a for loop.\n",
    "            *   `return pd.Series(predictions)`: Returns the results as a pandas Series.\n",
    "        *   `spark_df = spark_df.withColumn(\"predicted_target\", predict_with_model(col(\"feature_1\"), col(\"feature_2\")))`: Adds a new column named `predicted_target` to the Spark DataFrame. It applies the Pandas UDF to batches of rows using Spark column operations. This operation is distributed across the Spark cluster, allowing scalable predictions.\n",
    "\n",
    "5.  **Classification Metrics with Spark**:\n",
    "    *   **Purpose**: To evaluate the performance of the model on the distributed Spark DataFrame using classification metrics.\n",
    "    *   **Implementation**:\n",
    "        *   `predictions_pd = spark_df.select(\"target\", \"predicted_target\").toPandas()`: Transfers the target and predicted target columns to a pandas DataFrame.\n",
    "        *    `class_report = classification_report(predictions_pd['target'], predictions_pd['predicted_target'])`: Computes the classification report using the target and predicted target columns of the pandas DataFrame. The classification report contains metrics like precision, recall, F1-score, and support for each class in a classification problem.\n",
    "        *    `conf_matrix = confusion_matrix(predictions_pd['target'], predictions_pd['predicted_target'])`: Computes the confusion matrix which shows the performance of the classification model by showing the number of true positives, true negatives, false positives, and false negatives..\n",
    "\n",
    "**Key Concepts for First-Year Graduate Students:**\n",
    "\n",
    "*   **Distributed Data Processing:** Spark allows you to process large datasets across a cluster of computers, offering significant speedups over processing data on a single machine.\n",
    "*   **Pandas UDFs:** Enable you to leverage the power of Pandas and scikit-learn within Spark pipelines, combining the benefits of both libraries and enabling scalable model inference.\n",
    "*   **Ensemble Learning:** Random Forests are a type of ensemble learning method that combines the predictions of multiple decision trees to create a robust predictive model.\n",
    "*   **Classification Metrics:** Understanding the metrics like accuracy, recall, precision, and the confusion matrix is crucial for evaluating the performance of classification models.\n",
    "*   **Schema Definition:** Defining a schema when creating DataFrames (both in Spark and Pandas) is critical for understanding the structure of the data.\n",
    "\n",
    "**In summary,** this code demonstrates how to integrate scikit-learn, a library for machine learning algorithms, with Apache Spark, a system for large scale data processing. The process involves generating data, training a classification model, and applying it in a distributed way to new data, which is a typical process in modern machine learning workflows, while also calculating the quality of the predictions with different metrics. The use of Pandas UDF and distributed processing shows an efficient way of using data with different libraries in a scalable way.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "e784d49a-a681-4062-bf88-c99acd5f7b63",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.ensemble import RandomForestClassifier\n",
    "from sklearn.metrics import accuracy_score\n",
    "from pyspark.sql.functions import pandas_udf, col\n",
    "from pyspark.sql.types import FloatType, StringType, StructType, StructField\n",
    "from sklearn.metrics import classification_report, confusion_matrix\n",
    "\n",
    "# 1. Generate a Sample Dataset (using the same code as before)\n",
    "# ----------------------------\n",
    "np.random.seed(42)  # for reproducibility\n",
    "num_samples = 1000\n",
    "\n",
    "feature_1 = np.random.rand(num_samples) * 10\n",
    "feature_2 = np.random.rand(num_samples) * 10\n",
    "target = np.where(feature_1 + feature_2 > 10, \"Class_A\", \"Class_B\")\n",
    "data = pd.DataFrame({'feature_1': feature_1, 'feature_2': feature_2, 'target': target})\n",
    "\n",
    "# 2. Build an SKLearn Classification Model (using the same code as before)\n",
    "# ---------------------------------------\n",
    "X = data[['feature_1', 'feature_2']]\n",
    "y = data['target']\n",
    "\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)\n",
    "model = RandomForestClassifier(n_estimators=100, random_state=42)\n",
    "model.fit(X_train, y_train)\n",
    "predictions = model.predict(X_test)\n",
    "accuracy = accuracy_score(y_test, predictions)\n",
    "print(f\"SKLearn Model Accuracy: {accuracy}\")\n",
    "\n",
    "# 4. Create a Spark DataFrame\n",
    "# --------------------------\n",
    "\n",
    "# Define the schema for the spark dataframe\n",
    "schema = StructType([\n",
    "    StructField(\"feature_1\", FloatType(), True),\n",
    "    StructField(\"feature_2\", FloatType(), True),\n",
    "    StructField(\"target\", StringType(), True)\n",
    "])\n",
    "\n",
    "# create the spark dataframe using the pandas one\n",
    "spark_df = spark.createDataFrame(data, schema=schema)\n",
    "\n",
    "# Print the first few rows of the Spark DataFrame\n",
    "print(\"\\nSpark DataFrame:\")\n",
    "spark_df.show(5)\n",
    "\n",
    "# 5. Define a Pandas UDF for Inference\n",
    "# ------------------------------------\n",
    "\n",
    "# Define the input and output types\n",
    "@pandas_udf(\"string\")\n",
    "def predict_with_model(feature_1: pd.Series, feature_2: pd.Series) -> pd.Series:\n",
    "    # Create a pandas dataframe for input data\n",
    "    input_data = pd.DataFrame({'feature_1': feature_1, 'feature_2': feature_2})\n",
    "\n",
    "    # Get predictions from sklearn model using the input data\n",
    "    predictions = model.predict(input_data)\n",
    "    # returns the prediction for each row in the input.\n",
    "    return pd.Series(predictions)\n",
    "\n",
    "# 6. Perform Inference and Add Predictions\n",
    "# -----------------------------------------\n",
    "# Apply the Pandas UDF to get the predictions from the trained model.\n",
    "spark_df = spark_df.withColumn(\"predicted_target\", predict_with_model(col(\"feature_1\"), col(\"feature_2\")))\n",
    "\n",
    "print(\"\\nSpark DataFrame with Predictions:\")\n",
    "spark_df.show(5)\n",
    "\n",
    "# 7. Calculate Classification Statistics\n",
    "# ---------------------------------------\n",
    "\n",
    "# Convert Spark DataFrame to Pandas DataFrame for metrics calculation\n",
    "predictions_pd = spark_df.select(\"target\", \"predicted_target\").toPandas()\n",
    "\n",
    "# Get classification report and print it.\n",
    "class_report = classification_report(predictions_pd['target'], predictions_pd['predicted_target'])\n",
    "print(\"\\nClassification Report:\\n\", class_report)\n",
    "\n",
    "# Get confusion matrix and print it.\n",
    "conf_matrix = confusion_matrix(predictions_pd['target'], predictions_pd['predicted_target'])\n",
    "print(\"\\nConfusion Matrix:\\n\", conf_matrix)"
   ]
  }
 ],
 "metadata": {
  "application/vnd.databricks.v1+notebook": {
   "computePreferences": null,
   "dashboards": [],
   "environmentMetadata": {
    "base_environment": "",
    "environment_version": "2"
   },
   "inputWidgetPreferences": null,
   "language": "python",
   "notebookMetadata": {
    "mostRecentlyExecutedCommandWithImplicitDF": {
     "commandId": 7116861012130220,
     "dataframes": [
      "_sqldf"
     ]
    },
    "pythonIndentUnit": 4
   },
   "notebookName": "Spark UDF Overview",
   "widgets": {}
  },
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
